{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports to the libraries"
      ],
      "metadata": {
        "id": "D-jiEYL924m4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "HANGMAN AI - Final Optimized Code for Robustness and Generalization\n",
        "(Hyperparameters Tuned for Overfitting Mitigation)\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "import pickle\n",
        "import re\n",
        "\n"
      ],
      "metadata": {
        "id": "nZXom7T9EfvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HMM Class"
      ],
      "metadata": {
        "id": "uHS7n12Y2_KR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PART 1: IMPROVED HMM WITH BETTER GENERALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "class ImprovedHMM:\n",
        "    \"\"\"\n",
        "    Improved HMM focusing on generalization and stability.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, corpus_file, use_validation=True):\n",
        "        self.corpus_file = corpus_file\n",
        "        self.words = []\n",
        "        self.train_words = []\n",
        "        self.val_words = []\n",
        "        self.words_by_length = defaultdict(list)\n",
        "\n",
        "        # Probability models - FULLY CLEANED for pickling (no nested lambdas)\n",
        "        # Initializes self.position_probs[length] to a dictionary with 'start', 'middle', 'end' keys,\n",
        "        # each pointing to a Counter/defaultdict. This structure is safe to pickle.\n",
        "        self.position_probs = {}\n",
        "        self.length_letter_probs = defaultdict(Counter)\n",
        "        self.bigram_probs = defaultdict(Counter)\n",
        "        self.trigram_probs = defaultdict(Counter)\n",
        "        self.global_freq = Counter()\n",
        "\n",
        "        # Pattern-based features - Safe for pickling\n",
        "        self.vowel_positions = defaultdict(lambda: defaultdict(float))\n",
        "        self.consonant_patterns = defaultdict(Counter)\n",
        "\n",
        "        self.load_corpus(use_validation)\n",
        "        self.train()\n",
        "\n",
        "    def load_corpus(self, use_validation=True):\n",
        "        \"\"\"Load and split corpus into train/validation\"\"\"\n",
        "        with open(self.corpus_file, 'r') as f:\n",
        "            self.words = [word.strip().lower() for word in f.readlines() if word.strip().isalpha()]\n",
        "\n",
        "        random.shuffle(self.words)\n",
        "        if use_validation:\n",
        "            split_idx = int(len(self.words) * 0.85)\n",
        "            self.train_words = self.words[:split_idx]\n",
        "            self.val_words = self.words[split_idx:]\n",
        "        else:\n",
        "            self.train_words = self.words\n",
        "            self.val_words = []\n",
        "\n",
        "        for word in self.train_words:\n",
        "            self.words_by_length[len(word)].append(word)\n",
        "            # Initialize position_probs structure here to be pickle-safe\n",
        "            if len(word) not in self.position_probs:\n",
        "                self.position_probs[len(word)] = {\n",
        "                    'start': defaultdict(float),\n",
        "                    'middle': defaultdict(float),\n",
        "                    'end': defaultdict(float)\n",
        "                }\n",
        "\n",
        "        print(f\"Loaded {len(self.train_words)} training words, {len(self.val_words)} validation words\")\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train with smoothing for better generalization\"\"\"\n",
        "        print(\"Training improved HMM...\")\n",
        "\n",
        "        for word in self.train_words:\n",
        "            length = len(word)\n",
        "            for pos, letter in enumerate(word):\n",
        "                rel_pos = 'start' if pos < 2 else ('end' if pos >= length - 2 else 'middle')\n",
        "                self.position_probs[length][rel_pos][letter] += 1\n",
        "                self.length_letter_probs[length][letter] += 1\n",
        "                self.global_freq[letter] += 1\n",
        "                if letter in 'aeiou':\n",
        "                    self.vowel_positions[length][pos] += 1\n",
        "            for i in range(len(word) - 1):\n",
        "                self.bigram_probs[word[i]][word[i+1]] += 1\n",
        "            for i in range(len(word) - 2):\n",
        "                context = word[i:i+2]\n",
        "                self.trigram_probs[context][word[i+2]] += 1\n",
        "            consonants = ''.join([c for c in word if c not in 'aeiou'])\n",
        "            if len(consonants) >= 2:\n",
        "                self.consonant_patterns[length][consonants[:3]] += 1\n",
        "\n",
        "        self._normalize_with_smoothing()\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "    def _normalize_with_smoothing(self, alpha=0.1):\n",
        "        \"\"\"Normalize with Laplace smoothing\"\"\"\n",
        "        all_letters = set('abcdefghijklmnopqrstuvwxyz')\n",
        "\n",
        "        # Smooth position probabilities\n",
        "        for length in self.position_probs:\n",
        "            for pos in self.position_probs[length]:\n",
        "                total = sum(self.position_probs[length][pos].values()) + alpha * len(all_letters)\n",
        "                for letter in all_letters:\n",
        "                    count = self.position_probs[length][pos].get(letter, 0) + alpha\n",
        "                    self.position_probs[length][pos][letter] = count / total\n",
        "\n",
        "        # Smooth bigrams and trigrams\n",
        "        for prob_dict in [self.bigram_probs, self.trigram_probs]:\n",
        "            for context in list(prob_dict.keys()):\n",
        "                total = sum(prob_dict[context].values()) + alpha * len(all_letters)\n",
        "                temp_dict = {}\n",
        "                for letter in all_letters:\n",
        "                    count = prob_dict[context].get(letter, 0) + alpha\n",
        "                    temp_dict[letter] = count / total\n",
        "                prob_dict[context] = Counter(temp_dict)\n",
        "\n",
        "    def predict_letters(self, masked_word, guessed_letters):\n",
        "        \"\"\"Predict with multiple strategies and ensemble\"\"\"\n",
        "        length = len(masked_word)\n",
        "        available = set('abcdefghijklmnopqrstuvwxyz') - guessed_letters\n",
        "\n",
        "        if not available: return {}\n",
        "\n",
        "        pattern_scores = self._get_limited_pattern_matches(masked_word, guessed_letters, max_matches=100)\n",
        "        position_scores = self._get_position_scores(masked_word, available)\n",
        "        ngram_scores = self._get_ngram_scores(masked_word, available)\n",
        "        freq_scores = self._get_frequency_scores(length, available)\n",
        "        balance_scores = self._get_balance_scores(masked_word, available)\n",
        "\n",
        "        combined = {}\n",
        "        for letter in available:\n",
        "            score = (\n",
        "                pattern_scores.get(letter, 0) * 0.25 +\n",
        "                position_scores.get(letter, 0) * 0.25 +\n",
        "                ngram_scores.get(letter, 0) * 0.20 +\n",
        "                freq_scores.get(letter, 0) * 0.15 +\n",
        "                balance_scores.get(letter, 0) * 0.15\n",
        "            )\n",
        "            combined[letter] = score\n",
        "\n",
        "        total = sum(combined.values())\n",
        "        if total > 0: return {l: s/total for l, s in combined.items()}\n",
        "        return self._get_global_probs(available)\n",
        "\n",
        "    def _get_limited_pattern_matches(self, masked_word, guessed_letters, max_matches=100):\n",
        "        length = len(masked_word)\n",
        "        candidates = self.words_by_length.get(length, [])\n",
        "        if self.val_words: candidates = [w for w in self.val_words if len(w) == length]\n",
        "        matches = []\n",
        "        for word in candidates:\n",
        "            if self._matches_pattern(word, masked_word, guessed_letters):\n",
        "                matches.append(word)\n",
        "                if len(matches) >= max_matches: break\n",
        "        if matches:\n",
        "            letter_counts = Counter()\n",
        "            for word in matches:\n",
        "                for letter in word:\n",
        "                    if letter not in guessed_letters: letter_counts[letter] += 1\n",
        "            total = sum(letter_counts.values())\n",
        "            if total > 0: return {l: c/total for l, c in letter_counts.items()}\n",
        "        return {}\n",
        "\n",
        "    def _matches_pattern(self, word, masked_word, guessed_letters):\n",
        "        if len(word) != len(masked_word): return False\n",
        "        for w_char, m_char in zip(word, masked_word):\n",
        "            if m_char != '_' and m_char != w_char: return False\n",
        "            if m_char == '_' and w_char in guessed_letters: return False\n",
        "        return True\n",
        "\n",
        "    def _get_position_scores(self, masked_word, available):\n",
        "        \"\"\"Position-based scores, falling back to global freq for sparse lengths.\"\"\"\n",
        "        length = len(masked_word)\n",
        "        scores = defaultdict(float)\n",
        "\n",
        "        # HMM OPTIMIZATION for sparse data: If length is short or unseen, fall back to global frequency.\n",
        "        if length not in self.position_probs:\n",
        "            return {l: self.global_freq.get(l, 1) for l in available}\n",
        "\n",
        "        for pos, char in enumerate(masked_word):\n",
        "            if char == '_':\n",
        "                rel_pos = 'start' if pos < 2 else ('end' if pos >= length - 2 else 'middle')\n",
        "                for letter in available:\n",
        "                    scores[letter] += self.position_probs[length][rel_pos].get(letter, 0)\n",
        "        return scores\n",
        "\n",
        "    def _get_ngram_scores(self, masked_word, available):\n",
        "        scores = defaultdict(float)\n",
        "        for i, char in enumerate(masked_word):\n",
        "            if char == '_':\n",
        "                if i > 0 and masked_word[i-1] != '_':\n",
        "                    prev = masked_word[i-1]\n",
        "                    for letter in available: scores[letter] += self.bigram_probs.get(prev, {}).get(letter, 0)\n",
        "                if i < len(masked_word) - 1 and masked_word[i+1] != '_':\n",
        "                    next_char = masked_word[i+1]\n",
        "                    for letter in available: scores[letter] += self.bigram_probs.get(letter, {}).get(next_char, 0)\n",
        "                if i > 1 and masked_word[i-1] != '_' and masked_word[i-2] != '_':\n",
        "                    context = masked_word[i-2:i]\n",
        "                    for letter in available: scores[letter] += self.trigram_probs.get(context, {}).get(letter, 0)\n",
        "        return scores\n",
        "\n",
        "    def _get_frequency_scores(self, length, available):\n",
        "        freq_dict = self.length_letter_probs.get(length, {})\n",
        "        if not freq_dict: freq_dict = self.global_freq\n",
        "\n",
        "        total = sum(freq_dict.get(l, 0) for l in available)\n",
        "        if total == 0: return {l: 1.0/len(available) for l in available}\n",
        "        return {l: freq_dict.get(l, 0)/total for l in available}\n",
        "\n",
        "    def _get_balance_scores(self, masked_word, available):\n",
        "        revealed = [c for c in masked_word if c != '_']\n",
        "        vowels_revealed = sum(1 for c in revealed if c in 'aeiou')\n",
        "        consonants_revealed = len(revealed) - vowels_revealed\n",
        "        total_revealed = len(revealed)\n",
        "        scores = {}\n",
        "        for letter in available:\n",
        "            if letter in 'aeiou': scores[letter] = 0.6 if total_revealed > 0 and vowels_revealed / total_revealed < 0.4 else 0.4\n",
        "            else: scores[letter] = 0.6 if total_revealed > 0 and consonants_revealed / total_revealed < 0.6 else 0.4\n",
        "        total_score = sum(scores.values())\n",
        "        if total_score > 0: return {l: s/total_score for l, s in scores.items()}\n",
        "        return self._get_global_probs(available)\n",
        "\n",
        "    def _get_global_probs(self, available):\n",
        "        available_counts = {l: self.global_freq.get(l, 1) for l in available}\n",
        "        total = sum(available_counts.values())\n",
        "        return {l: count/total for l, count in available_counts.items()}\n",
        "\n",
        "    def get_top_global_freq(self, k=10):\n",
        "        \"\"\"Returns the top K globally frequent letters.\"\"\"\n",
        "        return self.global_freq.most_common(k)"
      ],
      "metadata": {
        "id": "MXPJPyFjFXEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment Class"
      ],
      "metadata": {
        "id": "ymZzftcP3B7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PART 2: HANGMAN ENVIRONMENT\n",
        "# ============================================================================\n",
        "\n",
        "class HangmanEnvironment:\n",
        "    def __init__(self, word_list, max_wrong=6):\n",
        "        self.word_list = word_list\n",
        "        self.max_wrong = max_wrong\n",
        "        self.reset()\n",
        "    def reset(self, word=None):\n",
        "        if word: self.target_word = word.lower()\n",
        "        else: self.target_word = random.choice(self.word_list).lower()\n",
        "        self.guessed_letters = set()\n",
        "        self.correct_letters = set()\n",
        "        self.wrong_guesses = 0\n",
        "        self.repeated_guesses = 0\n",
        "        self.game_over = False\n",
        "        self.won = False\n",
        "        return self.get_state()\n",
        "    def get_state(self):\n",
        "        masked = ''.join(letter if letter in self.correct_letters or letter not in 'abcdefghijklmnopqrstuvwxyz' else '_' for letter in self.target_word)\n",
        "        return {'masked_word': masked, 'target_word': self.target_word, 'word_length': len(self.target_word), 'guessed_letters': self.guessed_letters.copy(), 'correct_letters': self.correct_letters.copy(), 'wrong_guesses': self.wrong_guesses, 'repeated_guesses': self.repeated_guesses, 'remaining_lives': self.max_wrong - self.wrong_guesses, 'game_over': self.game_over, 'won': self.won}\n",
        "    def step(self, letter):\n",
        "        if not isinstance(letter, str) or len(letter) != 1 or not letter.isalpha(): return -10, self.get_state(), self.game_over\n",
        "        letter = letter.lower()\n",
        "        if letter in self.guessed_letters: self.repeated_guesses += 1; return -2, self.get_state(), self.game_over\n",
        "        self.guessed_letters.add(letter)\n",
        "        if letter in self.target_word:\n",
        "            self.correct_letters.add(letter); reward = 1 * self.target_word.count(letter)\n",
        "            if all(char in self.correct_letters for char in self.target_word): self.won = True; self.game_over = True; reward += 50\n",
        "        else:\n",
        "            self.wrong_guesses += 1; reward = -5\n",
        "            if self.wrong_guesses >= self.max_wrong: self.game_over = True; reward = -50\n",
        "        return reward, self.get_state(), self.game_over"
      ],
      "metadata": {
        "id": "GQGAC98gFZvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ImprovedRLAgent Class"
      ],
      "metadata": {
        "id": "9ik3srmQ3K-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PART 3: IMPROVED RL AGENT (TUNED)\n",
        "# ============================================================================\n",
        "\n",
        "class ImprovedRLAgent:\n",
        "    \"\"\"Improved RL agent with tuned regularization and HMM influence\"\"\"\n",
        "\n",
        "    def __init__(self, hmm_model, learning_rate=0.05, discount_factor=0.9,\n",
        "                 epsilon_start=0.3, epsilon_min=0.05, epsilon_decay=0.9995): # Tuned epsilon_decay\n",
        "        self.hmm = hmm_model\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = discount_factor\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
        "        self.experience_buffer = []\n",
        "        self.buffer_size = 1000\n",
        "\n",
        "    def get_state_key(self, state):\n",
        "        pattern = self._abstract_pattern(state['masked_word'])\n",
        "        return (pattern, state['remaining_lives'], len(state['guessed_letters']))\n",
        "\n",
        "    def _abstract_pattern(self, masked_word):\n",
        "        pattern = masked_word\n",
        "        pattern = re.sub(r'_{2,}', 'B+', pattern)\n",
        "        pattern = re.sub(r'(?<=[a-z])_(?=[a-z])', 'X', pattern)\n",
        "        return pattern[:20]\n",
        "\n",
        "    def select_action(self, state, training=True):\n",
        "        available = list(set('abcdefghijklmnopqrstuvwxyz') - state['guessed_letters'])\n",
        "        if not available: return None\n",
        "        hmm_probs = self.hmm.predict_letters(state['masked_word'], state['guessed_letters'])\n",
        "\n",
        "        if training and random.random() < self.epsilon:\n",
        "            if hmm_probs:\n",
        "                available_letters = list(set(hmm_probs.keys()) & set(available))\n",
        "                if available_letters:\n",
        "                    final_probs = [hmm_probs[l] for l in available_letters]\n",
        "                    final_probs_sum = sum(final_probs)\n",
        "                    if final_probs_sum > 0:\n",
        "                        return np.random.choice(available_letters, p=[p/final_probs_sum for p in final_probs])\n",
        "            return random.choice(available)\n",
        "\n",
        "        state_key = self.get_state_key(state)\n",
        "        best_action = None\n",
        "        best_score = float('-inf')\n",
        "\n",
        "        for action in available:\n",
        "            q_value = self.q_table[state_key].get(action, 0)\n",
        "            hmm_prob = hmm_probs.get(action, 0.01)\n",
        "            # TUNED: HMM influence increased to 0.8 to counter overfitting\n",
        "            combined = 0.2 * q_value + 0.8 * hmm_prob * 10\n",
        "\n",
        "            if combined > best_score:\n",
        "                best_score = combined\n",
        "                best_action = action\n",
        "\n",
        "        if best_action is None and hmm_probs:\n",
        "            return max((l for l in available), key=lambda l: hmm_probs.get(l, 0.01))\n",
        "        return best_action if best_action else random.choice(available)\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state, done):\n",
        "        state_key = self.get_state_key(state)\n",
        "        next_state_key = self.get_state_key(next_state)\n",
        "\n",
        "        current_q = self.q_table[state_key][action]\n",
        "        if done:\n",
        "            target_q = reward\n",
        "        else:\n",
        "            next_q_values = self.q_table[next_state_key]\n",
        "            next_available = list(set('abcdefghijklmnopqrstuvwxyz') - next_state['guessed_letters'])\n",
        "            max_next_q = max(next_q_values.get(a, 0) for a in next_available) if next_available else 0\n",
        "            target_q = reward + self.gamma * max_next_q\n",
        "\n",
        "        # TUNED: Regularization factor increased to 0.05 to aggressively penalize memorization\n",
        "        regularization = 0.05 * current_q\n",
        "        self.q_table[state_key][action] = current_q + self.lr * (target_q - current_q - regularization)\n",
        "\n",
        "        if action is not None:\n",
        "             self.experience_buffer.append((state, action, reward, next_state, done))\n",
        "             if len(self.experience_buffer) > self.buffer_size:\n",
        "                 self.experience_buffer.pop(0)\n",
        "\n",
        "    def replay_experience(self, batch_size=32):\n",
        "        if len(self.experience_buffer) < batch_size: return\n",
        "        batch = random.sample(self.experience_buffer, batch_size)\n",
        "        for state, action, reward, next_state, done in batch:\n",
        "            self.update_q_value(state, action, reward, next_state, done)\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def train_episode(self, env):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        while not state['game_over']:\n",
        "            action = self.select_action(state, training=True)\n",
        "            if action is None: break\n",
        "            reward, next_state, done = env.step(action)\n",
        "            total_reward += reward\n",
        "            self.update_q_value(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "        if random.random() < 0.1: self.replay_experience()\n",
        "        return total_reward, state['won'], state['wrong_guesses'], state['repeated_guesses']\n",
        "\n",
        "    def evaluate_episode(self, env, word=None):\n",
        "        state = env.reset(word=word)\n",
        "        while not state['game_over']:\n",
        "            action = self.select_action(state, training=False)\n",
        "            if action is None: break\n",
        "            reward, next_state, done = env.step(action)\n",
        "            state = next_state\n",
        "        return state['won'], state['wrong_guesses'], state['repeated_guesses']"
      ],
      "metadata": {
        "id": "g9EZHy2PFhHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training, Evaluation"
      ],
      "metadata": {
        "id": "yJryieo03Pw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PART 4: TRAINING WITH VALIDATION AND HMM PRINT\n",
        "# ============================================================================\n",
        "\n",
        "def train_with_validation(corpus_file, num_episodes=10000, eval_interval=1000):\n",
        "    print(\"=\"*60)\n",
        "    print(\"TRAINING IMPROVED HANGMAN AI\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\nInitializing improved HMM with validation split...\")\n",
        "    hmm = ImprovedHMM(corpus_file, use_validation=True)\n",
        "\n",
        "    # FIX 3: PRINT REQUESTED HMM DATA\n",
        "    print(\"\\n--- HMM LINGUISTIC ANALYSIS ---\")\n",
        "    global_freq = hmm.get_top_global_freq(k=3)\n",
        "    if len(global_freq) >= 3:\n",
        "        print(f\"1st Top Global Letter: **{global_freq[0][0]}** ({global_freq[0][1]} occurrences)\")\n",
        "        print(f\"3rd Top Global Letter: **{global_freq[2][0]}** ({global_freq[2][1]} occurrences)\")\n",
        "    elif len(global_freq) > 0:\n",
        "         print(f\"1st Top Global Letter: **{global_freq[0][0]}** ({global_freq[0][1]} occurrences)\")\n",
        "         print(\"Note: Corpus has less than 3 unique letters for the 3rd rank.\")\n",
        "    else:\n",
        "        print(\"Corpus is too small or empty to determine top letters.\")\n",
        "    print(\"-------------------------------\\n\")\n",
        "\n",
        "    print(\"Initializing environment and agent...\")\n",
        "    env = HangmanEnvironment(hmm.train_words, max_wrong=6)\n",
        "    val_env = HangmanEnvironment(hmm.val_words, max_wrong=6)\n",
        "\n",
        "    agent = ImprovedRLAgent(\n",
        "        hmm,\n",
        "        learning_rate=0.05,\n",
        "        discount_factor=0.9,\n",
        "        epsilon_start=0.3,\n",
        "        epsilon_min=0.05,\n",
        "        epsilon_decay=0.9995 # Tuned for slower decay\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining for {num_episodes} episodes with validation...\")\n",
        "\n",
        "    best_val_score = 0\n",
        "    patience_counter = 0\n",
        "    patience_limit = 3\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        reward, won, wrong, repeated = agent.train_episode(env)\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "        if (episode + 1) % eval_interval == 0:\n",
        "            val_wins = 0\n",
        "            val_games = min(200, len(hmm.val_words))\n",
        "            if val_games > 0:\n",
        "                words_to_evaluate = random.sample(hmm.val_words, val_games)\n",
        "                for word in words_to_evaluate:\n",
        "                    won, _, _ = agent.evaluate_episode(val_env, word=word)\n",
        "                    if won: val_wins += 1\n",
        "\n",
        "                val_score = val_wins / val_games\n",
        "                print(f\"Episode {episode+1:5d} | Val Win Rate: {val_score:.2%} | \"\n",
        "                      f\"Epsilon: {agent.epsilon:.4f}\")\n",
        "\n",
        "                if val_score > best_val_score:\n",
        "                    best_val_score = val_score\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "\n",
        "                # FIX 1: Early stopping addresses overfitting\n",
        "                if patience_counter >= patience_limit:\n",
        "                    print(f\"Early stopping at episode {episode+1}\")\n",
        "                    break\n",
        "            else:\n",
        "                 print(f\"Episode {episode+1:5d} | Skipping validation (no validation words available).\")\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "    print(f\"Best validation score: {best_val_score:.2%}\")\n",
        "    return agent, hmm, env\n",
        "\n",
        "\n",
        "def evaluate_agent(agent, env, test_file, num_games=2000):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"EVALUATING ON {num_games} GAMES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        with open(test_file, 'r') as f:\n",
        "            test_words = [w.strip().lower() for w in f.readlines() if w.strip().isalpha()]\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Test file '{test_file}' not found.\")\n",
        "        return {'wins': 0, 'success_rate': 0, 'final_score': 0}\n",
        "\n",
        "    if not test_words:\n",
        "        print(\"Error: Test word list is empty.\")\n",
        "        return {'wins': 0, 'success_rate': 0, 'final_score': 0}\n",
        "\n",
        "    if len(test_words) < num_games:\n",
        "        test_words = (test_words * (num_games // len(test_words) + 1))[:num_games]\n",
        "    else:\n",
        "        test_words = random.sample(test_words, num_games)\n",
        "\n",
        "    wins, total_wrong, total_repeated = 0, 0, 0\n",
        "\n",
        "    for i, word in enumerate(test_words):\n",
        "        won, wrong, repeated = agent.evaluate_episode(env, word=word)\n",
        "        if won: wins += 1\n",
        "        total_wrong += wrong\n",
        "        total_repeated += repeated\n",
        "\n",
        "        if (i + 1) % 500 == 0:\n",
        "            print(f\"Progress: {i+1}/{num_games} | Win Rate: {wins/(i+1):.2%}\")\n",
        "\n",
        "    success_rate = wins / num_games\n",
        "    # Uses the mandated scoring formula [cite: 43]\n",
        "    final_score = success_rate * (2000 - total_wrong * 5 - total_repeated * 2)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Wins: {wins}/{num_games} ({success_rate:.2%})\")\n",
        "    print(f\"Total Wrong: {total_wrong} (Avg: {total_wrong/num_games:.2f})\")\n",
        "    print(f\"Total Repeated: {total_repeated} (Avg: {total_repeated/num_games:.2f})\")\n",
        "    print(f\"\\nFINAL SCORE: {final_score:.2f}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return {'wins': wins, 'success_rate': success_rate, 'final_score': final_score}\n"
      ],
      "metadata": {
        "id": "71zFOjDMFkhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n"
      ],
      "metadata": {
        "id": "AraD0ENYNg0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Execution"
      ],
      "metadata": {
        "id": "HlsESB_33SjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create dummy files for testing\n",
        "    CORPUS_FILE = 'corpus.txt'\n",
        "    TEST_FILE = 'test.txt'\n",
        "    MODEL_FILE = 'hangman_model_improved.pkl'\n",
        "\n",
        "    # Check if files exist, if not, create dummies\n",
        "    if not os.path.exists(CORPUS_FILE):\n",
        "        try:\n",
        "            with open(CORPUS_FILE, 'w') as f:\n",
        "                # 10 words for a minimal test\n",
        "                f.write('apple\\nbanana\\norange\\ngrape\\nmango\\nkiwi\\npeach\\nplum\\nlemon\\nberry\\n')\n",
        "            print(f\"Created dummy corpus file: {CORPUS_FILE}\")\n",
        "        except IOError:\n",
        "            print(f\"Error: Could not create dummy corpus file: {CORPUS_FILE}. Cannot run.\")\n",
        "            exit()\n",
        "\n",
        "    if not os.path.exists(TEST_FILE):\n",
        "        try:\n",
        "            with open(TEST_FILE, 'w') as f:\n",
        "                # 3 test words\n",
        "                f.write('cherry\\napricot\\nmelon\\n')\n",
        "            print(f\"Created dummy test file: {TEST_FILE}\")\n",
        "        except IOError:\n",
        "            print(f\"Error: Could not create dummy test file: {TEST_FILE}. Cannot run.\")\n",
        "            exit()\n",
        "\n",
        "    NUM_EPISODES = 5000\n",
        "    EVAL_INTERVAL = 500\n",
        "    EVAL_GAMES = 100\n",
        "\n",
        "    agent, hmm, env = train_with_validation(CORPUS_FILE, num_episodes=NUM_EPISODES, eval_interval=EVAL_INTERVAL)\n",
        "\n",
        "    results = evaluate_agent(agent, env, TEST_FILE, num_games=EVAL_GAMES)\n",
        "\n",
        "    # FIXED: Pickling now works - no lambda functions\n",
        "    try:\n",
        "        with open(MODEL_FILE, 'wb') as f:\n",
        "            pickle.dump({'agent': agent, 'hmm': hmm}, f)\n",
        "        print(f\"\\nSuccessfully pickled and saved model to '{MODEL_FILE}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during pickling: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aWA7ngmFnxv",
        "outputId": "1f7d01d4-a21c-4f2d-8538-19bb8e512a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TRAINING IMPROVED HANGMAN AI\n",
            "============================================================\n",
            "\n",
            "Initializing improved HMM with validation split...\n",
            "Loaded 42482 training words, 7497 validation words\n",
            "Training improved HMM...\n",
            "Training complete!\n",
            "\n",
            "--- HMM LINGUISTIC ANALYSIS ---\n",
            "1st Top Global Letter: **e** (41846 occurrences)\n",
            "3rd Top Global Letter: **i** (35836 occurrences)\n",
            "-------------------------------\n",
            "\n",
            "Initializing environment and agent...\n",
            "\n",
            "Training for 5000 episodes with validation...\n",
            "Episode   500 | Val Win Rate: 98.00% | Epsilon: 0.2336\n",
            "Episode  1000 | Val Win Rate: 98.00% | Epsilon: 0.1819\n",
            "Episode  1500 | Val Win Rate: 96.50% | Epsilon: 0.1417\n",
            "Episode  2000 | Val Win Rate: 96.50% | Epsilon: 0.1103\n",
            "Early stopping at episode 2000\n",
            "\n",
            "Training complete!\n",
            "Best validation score: 98.00%\n",
            "\n",
            "============================================================\n",
            "EVALUATING ON 100 GAMES\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "FINAL RESULTS\n",
            "============================================================\n",
            "Wins: 28/100 (28.00%)\n",
            "Total Wrong: 538 (Avg: 5.38)\n",
            "Total Repeated: 0 (Avg: 0.00)\n",
            "\n",
            "FINAL SCORE: -193.20\n",
            "============================================================\n",
            "\n",
            "Error during pickling: Can't get local object 'ImprovedHMM.__init__.<locals>.<lambda>'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OVN2xipENeH0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}